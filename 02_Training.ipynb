{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Training\n",
        "\n",
        "In this notebook, we will learn how to train an AI model in the cloud.  \n",
        "There are a few things that are special regarding Cloud AI training, but also a lot of similarities between our old-school way of working."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Again let us start by setting some global parameters first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1670594149135
        }
      },
      "outputs": [],
      "source": [
        "INITIAL_LEARNING_RATE = 0.01\n",
        "MAX_EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 11\n",
        "model_name = 'rice-cnn'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "And of course importing the packages we need! Again, don't forget to set your kernel right in the top-right corner!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1670594150027
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "SEED = 42   # set random seed\n",
        "random.seed(SEED)\n",
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1670594150838
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "## Import AzureML packages\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "One special import are these Utils scripts. You can read more about them in the `utils > utils.py` file. I have included them here to load them in. They contain some helper functions we will be needing later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1670594151896
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from utils.utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 1: Connect Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Follow the same steps as the previous notebook, to set up your Workspace configuration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1670594154383
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "## Either get environment variables, or a fallback name, which is the second parameter.\n",
        "## Currently, fill in the fallback values. Later on, we will make sure to work with Environment values. So we're already preparing for it in here!\n",
        "workspace_name = os.environ.get('WORKSPACE', 'felix-platteau-ml')\n",
        "subscription_id = os.environ.get('SUBSCRIPTION_ID', '9ae53766-1df1-4559-8ad6-262ddacf26b7')\n",
        "resource_group = os.environ.get('RESOURCE_GROUP', 'mlops')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1670594155414
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.get(name=workspace_name,\n",
        "               subscription_id=subscription_id,\n",
        "               resource_group=resource_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 1.1 -- Create Compute Cluster\n",
        "\n",
        "A Compute Cluster is a combination of multiple Compute Instances. Azure will scale these machines according to the number of nodes we fill into the configuration.  \n",
        "Based on the amount of Jobs we want to run in parallel, multiple machines will be created.\n",
        "\n",
        "We choose to define a minimum of 0 machines, which means Azure will need some time to create at least one machine everytime we need one.\n",
        "If you keep the minimum on 1, you always have one that's ready for your development.\n",
        "The timeout time to scale down back to 0 machines can also be configured if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670583334300
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# choose a name for your cluster\n",
        "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
        "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
        "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
        "\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
        "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
        "\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print(\"found compute target: \" + compute_name)\n",
        "else:\n",
        "    print(\"creating new compute target...\")\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
        "                                                                min_nodes = compute_min_nodes, \n",
        "                                                                max_nodes = compute_max_nodes)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    \n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Find and download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1670594160418
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ 'Arborio_v1': DatasetRegistration(id='23aa2c70-2e22-4f07-aa02-fd8b6f3c10ba', name='Arborio_v1', version=1, description='', tags={}),\n",
            "  'Basmati_v1': DatasetRegistration(id='d9c164c7-a5a1-4f6d-a050-4e4ff1c2f35a', name='Basmati_v1', version=1, description='', tags={}),\n",
            "  'Ipsala_v1': DatasetRegistration(id='0bf0349b-ac3b-4459-95ba-e0f6fdc5da38', name='Ipsala_v1', version=1, description='', tags={}),\n",
            "  'Jasmine_v1': DatasetRegistration(id='67f20d87-a70d-4657-92a9-5a83a0e3711a', name='Jasmine_v1', version=1, description='', tags={}),\n",
            "  'Karacadag_v1': DatasetRegistration(id='1a7c3105-f8b1-43fd-956d-a1d3b20d4f58', name='Karacadag_v1', version=1, description='', tags={}),\n",
            "  'resized_Arborio_v1': DatasetRegistration(id='e3d9bc12-0f83-468a-9ac1-7e7219124f6b', name='resized_Arborio_v1', version=1, description='Arborio_v1 images resized tot 64, 64', tags={'rice': 'Arborio_v1', 'AI-Model': 'CNN'}),\n",
            "  'resized_Basmati_v1': DatasetRegistration(id='4a5e3c95-f4ec-4942-b764-6331588217bb', name='resized_Basmati_v1', version=1, description='Basmati_v1 images resized tot 64, 64', tags={'rice': 'Basmati_v1', 'AI-Model': 'CNN'}),\n",
            "  'resized_Ipsala_v1': DatasetRegistration(id='4ab6790b-ec28-4a3c-b397-71275f58d112', name='resized_Ipsala_v1', version=1, description='Ipsala_v1 images resized tot 64, 64', tags={'rice': 'Ipsala_v1', 'AI-Model': 'CNN'}),\n",
            "  'resized_Jasmine_v1': DatasetRegistration(id='837a6eb4-93de-45cc-80ea-8b47dc81b2c8', name='resized_Jasmine_v1', version=1, description='Jasmine_v1 images resized tot 64, 64', tags={'rice': 'Jasmine_v1', 'AI-Model': 'CNN'}),\n",
            "  'resized_Karacadag_v1': DatasetRegistration(id='5376540b-f496-4031-b941-414a26696526', name='resized_Karacadag_v1', version=1, description='Karacadag_v1 images resized tot 64, 64', tags={'rice': 'Karacadag_v1', 'AI-Model': 'CNN'}),\n",
            "  'rice-testing-set': DatasetRegistration(id='6030219e-3c8c-41eb-9463-d562f9510165', name='rice-testing-set', version=1, description='The Rice Images to test, resized tot 64, 64', tags={'rice': 'Arborio_v1,Basmati_v1,Ipsala_v1,Jasmine_v1,Karacadag_v1', 'AI-Model': 'CNN', 'Split size': '0.2', 'type': 'testing'}),\n",
            "  'rice-training-set': DatasetRegistration(id='a026cd74-a2c6-4c01-8471-bc7c9d0bb518', name='rice-training-set', version=1, description='The Rice Images to train, resized tot 64, 64', tags={'rice': 'Arborio_v1,Basmati_v1,Ipsala_v1,Jasmine_v1,Karacadag_v1', 'AI-Model': 'CNN', 'Split size': '0.8', 'type': 'training'})}\n"
          ]
        }
      ],
      "source": [
        "datasets = Dataset.get_all(workspace=ws) # Make sure to give our workspace with it\n",
        "print(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 2: Create an AI model and training code\n",
        "\n",
        "We will first create an AI model to use in our training script.  \n",
        "A basic AI model has been given in the /utils/utils.py directory. You can change it there if you want to\n",
        "\n",
        "In this step, we will also configure a Training script. This script is an Executable Python script.  \n",
        "This is slightly different from our other way of working, where we work with Notebooks.\n",
        "\n",
        "Because Azure will be launching and running our Python scripts, we need to create one file that can be executed in one go.\n",
        "This needs all our imports, packages, data ... ready without manual interference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We'll store all of these files into a scripts directory. That way we can upload that directory to our training VM later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2.1 -- Prepare the scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670585170878
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "script_folder = os.path.join(os.getcwd(), 'scripts')\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile $script_folder/train.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "\n",
        "# This time we will need our Tensorflow Keras libraries, as we will be working with the AI training now\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# This AzureML package will allow to log our metrics etc.\n",
        "from azureml.core import Run\n",
        "\n",
        "# Important to load in the utils as well!\n",
        "from utils import *\n",
        "\n",
        "\n",
        "### HARDCODED VARIABLES FOR NOW\n",
        "### TODO for the students:\n",
        "### Make sure to adapt the ArgumentParser on line 31 to include these parameters\n",
        "### You can base your answer on the lines that are already there\n",
        "\n",
        "SEED = 42\n",
        "INITIAL_LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 11\n",
        "model_name = 'rice-cnn-test'\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--training-folder', type=str, dest='training_folder', help='training folder mounting point')\n",
        "parser.add_argument('--testing-folder', type=str, dest='testing_folder', help='testing folder mounting point')\n",
        "parser.add_argument('--epochs', type=int, dest='epochs', help='The amount of Epochs to train')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "training_folder = args.training_folder\n",
        "print('Training folder:', training_folder)\n",
        "\n",
        "testing_folder = args.testing_folder\n",
        "print('Testing folder:', testing_folder)\n",
        "\n",
        "MAX_EPOCHS = args.epochs\n",
        "\n",
        "# As we're mounting the training_folder and testing_folder onto the `/mnt/data` directories, we can load in the images by using glob.\n",
        "training_paths = glob(os.path.join('/mnt/data/train', '**', 'processed_rice', '**', '*.jpg'), recursive=True)\n",
        "testing_paths = glob(os.path.join('/mnt/data/test', '**', 'processed_rice', '**', '*.jpg'), recursive=True)\n",
        "\n",
        "print(\"Training samples:\", len(training_paths))\n",
        "print(\"Testing samples:\", len(testing_paths))\n",
        "\n",
        "# Make sure to shuffle in the same way as I'm doing everything\n",
        "random.seed(SEED)\n",
        "random.shuffle(training_paths)\n",
        "random.seed(SEED)\n",
        "random.shuffle(testing_paths)\n",
        "\n",
        "print(training_paths[:3]) # Examples\n",
        "print(testing_paths[:3]) # Examples\n",
        "\n",
        "# Parse to Features and Targets for both Training and Testing. Refer to the Utils package for more information\n",
        "X_train = getFeatures(training_paths)\n",
        "y_train = getTargets(training_paths)\n",
        "\n",
        "X_test = getFeatures(testing_paths)\n",
        "y_test = getTargets(testing_paths)\n",
        "\n",
        "print('Shapes:')\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(len(y_train))\n",
        "print(len(y_test))\n",
        "\n",
        "# Make sure the data is one-hot-encoded\n",
        "LABELS, y_train, y_test = encodeLabels(y_train, y_test)\n",
        "print('One Hot Shapes:')\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# Create an output directory where our AI model will be saved to.\n",
        "# Everything inside the `outputs` directory will be logged and kept aside for later usage.\n",
        "model_path = os.path.join('outputs', model_name)\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "## START OUR RUN context.\n",
        "## We can now log interesting information to Azure, by using these methods.\n",
        "run = Run.get_context()\n",
        "\n",
        "# Save the best model, not the last\n",
        "cb_save_best_model = keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
        "                                                         monitor='val_loss', \n",
        "                                                         save_best_only=True, \n",
        "                                                         verbose=1)\n",
        "\n",
        "# Early stop when the val_los isn't improving for PATIENCE epochs\n",
        "cb_early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                              patience= PATIENCE,\n",
        "                                              verbose=1,\n",
        "                                              restore_best_weights=True)\n",
        "\n",
        "# Reduce the Learning Rate when not learning more for 4 epochs.\n",
        "cb_reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=4, verbose=1)\n",
        "\n",
        "opt = SGD(learning_rate=INITIAL_LEARNING_RATE, momentum=INITIAL_LEARNING_RATE / MAX_EPOCHS) # Define the Optimizer\n",
        "\n",
        "model = buildModel((64, 64, 3), 3) # Create the AI model as defined in the utils script.\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Construct & initialize the image data generator for data augmentation\n",
        "# Image augmentation allows us to construct “additional” training data from our existing training data \n",
        "# by randomly rotating, shifting, shearing, zooming, and flipping. This is to avoid overfitting.\n",
        "# It also allows us to fit AI models using a Generator, so we don't need to capture the whole dataset in memory at once.\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "# train the network\n",
        "history = model.fit(aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "                        epochs=MAX_EPOCHS,\n",
        "                        callbacks=[cb_save_best_model, cb_early_stop, cb_reduce_lr_on_plateau] )\n",
        "\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "print(classification_report(y_test.argmax(axis=1), predictions.argmax(axis=1), target_names=['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag'])) # Give the target names to easier refer to them.\n",
        "# If you want, you can enter the target names as a parameter as well, in case you ever adapt your AI model to more rice.\n",
        "\n",
        "cf_matrix = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
        "print(cf_matrix)\n",
        "\n",
        "### TODO for students\n",
        "### Find a way to log more information to the Run context.\n",
        "\n",
        "# Save the confusion matrix to the outputs.\n",
        "np.save('outputs/confusion_matrix.npy', cf_matrix)\n",
        "\n",
        "print(\"DONE TRAINING\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670585176100
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Copy the Utils file into the script_folder\n",
        "import shutil\n",
        "shutil.copy('utils/utils.py', script_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2.2 -- Prepare the environment\n",
        "\n",
        "The training script we have just defined still needs some more information before we can start it.  \n",
        "We'll need to define it's Anaconda or Pip environment with all the packages that should be installed prior to training.  \n",
        "We can re-use the environments later, or we can use environments other people have created for us.\n",
        "\n",
        "You can also customize the Base Docker image to train on, if you prefer. I won't use this in here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670585179251
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# Create an Environment name for later use\n",
        "environment_name = os.environ.get('TRAINING_ENV_NAME', 'rice-classification-env-training')\n",
        "env = Environment(environment_name)\n",
        "\n",
        "# It's called CondaDependencies, but you can also use pip packages ;-)\n",
        "env.python.conda_dependencies = CondaDependencies.create(\n",
        "        # Using opencv-python-headless is interesting to skip the overhead of packages that we don't need in a headless-VM.\n",
        "        pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'scikit-learn', 'opencv-python-headless']\n",
        "    )\n",
        "# Register environment to re-use later\n",
        "env.register(workspace = ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2.3 -- Prepare the ScriptRunConfig\n",
        "\n",
        "A **ScriptRunConfig** is a configuration that contains all the information needed to launch a Job inside an Experiment.\n",
        "This contains information to the directory of scripts it should use, the **name** of the script to start,\n",
        "the **arguments** to pass into that script, the **compute** target to run the script on, and finally the **environment** to run it on.\n",
        "\n",
        "We then need to attach such a ScriptRunConfig onto an Experiment on Azure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670585191835
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.core import Experiment\n",
        "\n",
        "experiment_name = os.environ.get('EXPERIMENT_NAME', 'Rices-Classification')\n",
        "\n",
        "exp = Experiment(workspace=ws, name=experiment_name) # Create a new experiment\n",
        "\n",
        "experiment_runs = []\n",
        "\n",
        "# We can start four experiments for a bunch of different epoch options\n",
        "for epochs in [25, 50, 75, 100]:\n",
        "    args = [\n",
        "        '--training-folder', datasets['rice-training-set'].as_mount('/mnt/data/train'),\n",
        "        '--testing-folder', datasets['rice-testing-set'].as_mount('/mnt/data/test'),\n",
        "        '--epochs', epochs]\n",
        "\n",
        "    script_run_config = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script='train.py', \n",
        "                      arguments=args,\n",
        "                      compute_target=compute_target,\n",
        "                      environment=env)\n",
        "\n",
        "    run = exp.submit(config=script_run_config)\n",
        "    experiment_runs.append(run) # Append it to our list of experiment runs for now. This is easy for referring later!\n",
        "    print('Run started!')\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2.4 -- Await the results!\n",
        "\n",
        "Now that our experiment runs are starting, we can await the logs and results.  \n",
        "It can take a while to run everything, but the 4 jobs should run in Parallel, if all was well configured!\n",
        "\n",
        "The cells below can help you in viewing the results, while you head out for a coffee!\n",
        "\n",
        "I use the `experiment_runs[0]` as our run to log. It's the first one that was started.\n",
        "\n",
        "There are a few different options for each to select the one they prefer :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 2.4.1 -- Plain text output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670590883157
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# specify show_output to True for a verbose log\n",
        "experiment_runs[1].wait_for_completion(show_output=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 2.4.2 -- AzureML Widgets\n",
        "\n",
        "This needs an extra package to be installed, the AzureML widgets.\n",
        "(Change the environment if you're running this in a different Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!/anaconda/envs/azureml_py38_tensorflow/bin/python -m pip install azureml-widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670593993690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(experiment_runs[0]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 3 -- The Output!\n",
        "\n",
        "As a result of our experiments, we should have a trained AI model.  \n",
        "However, we have some more information that was logged or saved. You can find all this information in the Run context that we started, and Azure is filling in for us.\n",
        "\n",
        "Use the documentation to find out some more information.\n",
        "\n",
        "https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrun?view=azure-ml-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670593999744
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "experiment_name = os.environ.get('EXPERIMENT_NAME', 'Rices-Classification')\n",
        "\n",
        "exp = Experiment(workspace=ws, name=experiment_name) # re-use existing experiment with this name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594001480
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "runs = exp.get_runs()\n",
        "first_run = next(runs)\n",
        "second_run = next(runs)\n",
        "print(first_run)\n",
        "print(second_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594018971
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "second_run.get_file_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594025977
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run = experiment_runs[2]\n",
        "# Get the files that were logged\n",
        "print(run.get_file_names())\n",
        "\n",
        "# Get the metrics\n",
        "print(run.get_metrics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594033782
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# You can always save these details for further referencing!\n",
        "second_run.get_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594037519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get a list of all the runs, so you can take one specific run based on it's ID\n",
        "all_runs = list(exp.get_runs())\n",
        "\n",
        "print(all_runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 3.1 -- TODO: Finding the logs for a specific run based on it's ID.\n",
        "\n",
        "Try to create a function that find the logs for a specific run.  \n",
        "You only fill in the RunID number and you get the output you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1640007585777
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Uncomment the cell below for a possible answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594045964
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# %load code_solutions/ViewRunDetails.txt\n",
        "from azureml.core import Run\n",
        "test_run = Run(exp, 'Rices-Classification_1670585189_0727d024')\n",
        "test_run.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 3.2 -- Registering and downloading our AI model\n",
        "\n",
        "In Azure Machine Learning Service, we can register AI models so that they are versioned and kept together with other AI models.  \n",
        "We keep track of their accuracy based on the runs they were created from.\n",
        "\n",
        "Just a few lines are needed to register and download the AI model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594063548
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_path = 'outputs/rice-cnn-test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670594070600
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "test_run.download_files(prefix=model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1670594135050
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Model(workspace=Workspace.create(name='felix-platteau-ml', subscription_id='9ae53766-1df1-4559-8ad6-262ddacf26b7', resource_group='mlops'), name=rice-cnn, id=rice-cnn:2, version=2, tags={'rice': 'Arborio, Basmati, Ipsala, Jasmine, Karacadag', 'AI-Model': 'CNN'}, properties={})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_run.register_model(model_name,\n",
        "                model_path=model_path,\n",
        "                tags={'rice': 'Arborio, Basmati, Ipsala, Jasmine, Karacadag', 'AI-Model': 'CNN'},\n",
        "                description=\"Image classification on rice\",\n",
        "                sample_input_dataset=datasets['rice-testing-set'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf6434c5a1aa7c8e434dd054297a02391e6efb1cc4c519f6d78eae818bb65a96"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
